# -*- coding: utf-8 -*-
"""A3DEEPLEARNING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/178zwzdnQSL73Fzodo_WU8tixx4novWLE

# Helper Code for Assignment 3 (RNN language models)

## Reading raw text file & Create DataLoader
"""

import os
import torch
import torch.nn as nn

import numpy as np



class Vocabulary:

    def __init__(self, pad_token="<pad>", unk_token='<unk>'):
        self.id_to_string = {}
        self.string_to_id = {}
        
        # add the default pad token
        self.id_to_string[0] = pad_token
        self.string_to_id[pad_token] = 0
        
        # add the default unknown token
        self.id_to_string[1] = unk_token
        self.string_to_id[unk_token] = 1        
        
        # shortcut access
        self.pad_id = 0
        self.unk_id = 1
        
    def __len__(self):
        return len(self.id_to_string)

    def add_new_word(self, string):
        self.string_to_id[string] = len(self.string_to_id)
        self.id_to_string[len(self.id_to_string)] = string

    # Given a string, return ID
    def get_idx(self, string, extend_vocab=False):
        if string in self.string_to_id:
            return self.string_to_id[string]
        elif extend_vocab:  # add the new word
            self.add_new_word(string)
            return self.string_to_id[string]
        else:
            return self.unk_id


# Read the raw txt file and generate a 1D PyTorch tensor
# containing the whole text mapped to sequence of token IDs, and a vocab object.
class LongTextData:

    def __init__(self, file_path, vocab=None, extend_vocab=True, device='cuda'):
        self.data, self.vocab = self.text_to_data(file_path, vocab, extend_vocab, device)
        
    def __len__(self):
        return len(self.data)

    def text_to_data(self, text_file, vocab, extend_vocab, device):
        """Read a raw text file and create its tensor and the vocab.

        Args:
          text_file: a path to a raw text file.
          vocab: a Vocab object
          extend_vocab: bool, if True extend the vocab
          device: device

        Returns:
          Tensor representing the input text, vocab file

        """
        assert os.path.exists(text_file)
        if vocab is None:
            vocab = Vocabulary()

        data_list = []

        # Construct data
        full_text = []
        print(f"Reading text file from: {text_file}")
        count_line=0
        count_cap_lett=0
        contractions=0
        with open(text_file, 'r') as text:
            for line in text:
                count_line+=1
                tokens = list(line)
                print()
                i=-1
                for token in tokens:
                    i+=1
                    # print(token)
                    # get index will extend the vocab if the input
                    # token is not yet part of the text.
                    if(token.isupper()):
                        count_cap_lett+=1
                    if(i>0):
                        if((token=="'" or token=="`") and tokens[i-1].isalpha() and tokens[i+1].isalpha()):
                            contractions+=1
                    full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))
        print("number of lines: ", count_line)
        print("number of capitalized letters: ", count_cap_lett)
        print("number of contraction: ",contractions)
        # convert to tensor
        data = torch.tensor(full_text, device=device, dtype=torch.int64)
        print("Done.")

        return data, vocab
    

# Since there is no need for schuffling the data, we just have to split
# the text data according to the batch size and bptt length.
# The input to be fed to the model will be batch[:-1]
# The target to be used for the loss will be batch[1:]
class ChunkedTextData:

    def __init__(self, data, bsz, bptt_len, pad_id):
        self.batches = self.create_batch(data, bsz, bptt_len, pad_id)

    def __len__(self):
        return len(self.batches)

    def __getitem__(self, idx):
        return self.batches[idx]

    def create_batch(self, input_data, bsz, bptt_len, pad_id):
        """Create batches from a TextData object .

        Args:
          input_data: a TextData object.
          bsz: int, batch size
          bptt_len: int, bptt length
          pad_id: int, ID of the padding token

        Returns:
          List of tensors representing batches

        """
        batches = []  # each element in `batches` is (len, B) tensor
        text_len = len(input_data)
        print("text len:",text_len)
        segment_len = text_len // bsz + 1
        print("segnment len ", segment_len)

        # Question: Explain the next two lines!
        padded = input_data.data.new_full((segment_len * bsz,), pad_id)
        print("padded len: ", len(padded))
        print("padded= ",padded)
        #for i in padded[:6]:
        #    print(i)
            
        padded[:text_len] = input_data.data
        
        #print("padded= ",padded)
        #for i in padded:
        #    print(i)
        
        padded = padded.view(bsz, segment_len).t()
        print(padded)
        #print("padded= ",padded)
        
        #for i in padded[:6]:
        #    print(i)
        num_batches = segment_len // bptt_len + 1

        for i in range(num_batches):
            # Prepare batches such that the last symbol of the current batch
            # is the first symbol of the next batch.
            if i == 0:
                # Append a dummy start symbol using pad token
                batch = torch.cat(
                    [padded.new_full((1, bsz), pad_id),
                     padded[i * bptt_len:(i + 1) * bptt_len]], dim=0)
                batches.append(batch)
                #print(padded[i * bptt_len:(i + 1) * bptt_len].shape)
            else:
                batches.append(padded[i * bptt_len - 1:(i + 1) * bptt_len])
                #print(padded[i * bptt_len - 1:(i + 1) * bptt_len].shape)

        return batches

# downlaod the text
# Make sure to go to the link and check how the text looks like.

!wget http://www.gutenberg.org/files/49010/49010-0.txt

# This is for Colab. Adapt the path if needed.

#text_path = "/content/49010-0.txt"
#text_path = "./content/49010-0.txt"
text_path = "./49010-0.txt"

DEVICE = 'cuda'

batch_size = 32
bptt_len = 64
if torch.cuda.is_available():
    torch.device(DEVICE)
    print("using cuda")
else:
    DEVICE='cpu'
    torch.device('cpu')
    
my_data = LongTextData(text_path, device=DEVICE)
#print(" Length of Vocabolary: ", len(my_data.vocab))


batches = ChunkedTextData(my_data, batch_size, bptt_len, pad_id=0)
#print(len(batches))

len(batches)
print(len(batches))
print("len of data: : ",len(my_data))
print(my_data.data)

batches[0].shape

# input to the network
print(batches[0][:-1].shape)
batches[0][:-1, 0]

# target tokens to be predicted
print(batches[0][1:].shape)
batches[0][1:, 0]
print(batches[1].shape)
print(batches[86].shape)

# last token of the current batch should be the first token of the next one:
for i in batches[20][:, 11]:
    print(my_data.vocab.id_to_string[i.item()])
print('================')
for i in batches[21][:, 11]:
    print(my_data.vocab.id_to_string[i.item()])

print(my_data.vocab.id_to_string)
print(my_data.vocab.string_to_id)

"""# Taking the argmax vs. Sampling from a distribution"""

# Let's consider a "dice" with five faces a following probability:
# 0: 0.2
# 1: 0.1
# 2: 0.4
# 3: 0.2
# 4: 0.1

dice = torch.tensor([0.2, 0.1, 0.4, 0.2, 0.1], device=DEVICE)

# Sampling = roll dice
num_rolls = 5

for i in range(num_rolls):
    print(torch.multinomial(dice, num_samples=1))

class LSTMModel(nn.Module):
    def __init__(self, embed_size, hidden_size, num_layers, vocab_size):
        super(LSTMModel, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0) 
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)
        self.linear = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, h):
        
        x = self.embed(x)
        out, (h, c) = self.lstm(x, h)
        #print("calculation of out, h and c")
        out = out.reshape(out.size(0) * out.size(1), out.size(2))
        out = self.linear(out)
        return out, (h, c)

def trying_string(prompt, len_of_out_string, randomness, model, num_layers, batch_size, hidden_size):
  h = torch.zeros(num_layers, 1, hidden_size).to(DEVICE)
  c = torch.zeros(num_layers, 1, hidden_size).to(DEVICE)
  prompt=list(prompt)
  distribution=0
  out=0
  #model.eval()
  for i in range(len(prompt)):
    input=torch.ones((1,1)).new_full((1,1), my_data.vocab.string_to_id[prompt[i]], dtype=torch.int64).to(DEVICE)
    output, (h,c)=model(input, (h,c))
    distribution=torch.nn.functional.softmax(output, dim=1)

  if(randomness):
      out=torch.multinomial(distribution, num_samples=1).item()
  else:
      out=torch.argmax(distribution).item()
  
  for i in range(len_of_out_string):
    out=my_data.vocab.id_to_string[out]
    print(out, end='')
    input = torch.ones((1,1)).new_full((1,1), my_data.vocab.string_to_id[out], dtype=torch.int64).to(DEVICE)
    output, (h, c) = model(input, (h, c))
    distribution= torch.nn.functional.softmax(output, dim=1)
    if(randomness):
      out=torch.multinomial(distribution, num_samples=1).item()
    else:
      out=torch.argmax(distribution).item()

print("batch_size: ", batch_size)
print("we are using : ", )
perplexities=[]
ep=[]
batch_size=32
num_layers=1
hidden_size=2048

h = torch.zeros(num_layers, batch_size, hidden_size)
c = torch.zeros(num_layers, batch_size, hidden_size)
#h = h.to(device)
#c = c.to(device)
mylstm=LSTMModel(batch_size,2048, num_layers, len(my_data.vocab))
mylstm=mylstm.to(DEVICE)
h=h.to(DEVICE)
c=c.to(DEVICE)
num_epochs=30
criterion = nn.CrossEntropyLoss(ignore_index=0)
lr=0.001
optimizer = torch.optim.Adam(mylstm.parameters(), lr=lr)
count=0
best_epoch=0.0
best_loss=0.0
last_epoch=0.0
last_loss=0.0
perplexity=0.0
best_perplexity=0.0
mylstm.train()
perp=0

#string="TMODERN versions of Æsop go back no further than 480 A.D. 
#In their earliest\n use they are related to the folklore current among all primitive peoples.\n 
#This folklore had risen in Greece to the rank of literary form a thousand years before the above-mentioned revival"

#string="Dogs like best to"
string="THE FOX AND THE GRAPES"

# ******* STARTING TRAINING ************
for epoch in range(num_epochs):
    # for each batch
    for batch in batches:
        # taking the target and the inputs
        x=batch[:-1]
        y=batch[1:]
        #print("x: ", x)
        #print(x.shape)
        #print("y: ", y)
        #print(y.shape)
        # outpout, hidden state and cell state
        outputs, (h, c) = mylstm(x, (h, c)) # forward step
        #print(h)
        
        loss = criterion(outputs, y.flatten()) # getting our loss
        mylstm.zero_grad()
        loss.backward() # compute the backward

        nn.utils.clip_grad_norm_(mylstm.parameters(), 1.0)  # 
        optimizer.step()


        # detach the hhidden and the cell states
        h=h.detach()
        c=c.detach()

        perp=np.exp(loss.item()) # computing the perplexity
        #ep.append(epoch) # saving all epochs==> not necessary
        perplexities.append(perp) # save perplexity  
        if(best_perplexity>perp):
          best_perplexity=perp
          best_epoch=epoch


        # at each count%50, print results and compute greedy
        if(count%50==0):
          #print(outputs)
          #mylstm.eval()
          # printing our string 
          print(string, end='')
          # try our greedy algorithm
          trying_string(string, 40, True, mylstm, num_layers, batch_size, hidden_size)
          #mylstm.train()
          # print some parameters
          print("\nepoch: ", epoch)
          print("Loss: ", loss.item())
          print("perplexity : ", perp)
        count+=1 # increase our counting variable
        #step = (i + 1) // len(batches)
    # exiting if the perplexity is less than 1.03
    if(np.exp(loss.item())<1.03):
      print("exiting, perplexity less than 1.03")
      perplexity=np.exp(loss.item())
      last_epoch=epoch # save our last epoch
      last_loss=loss.item() # get our last loss
      break; # break when perplexity is less than 1.03

string="THE FOX AND THE GRAPES"
#string ="I am Aesop and "
#string='The Tortoise challenged Achilles to a race, claiming that he would win as long as Achilles gave him a small head start. Achilles laughed at this, for of course he was a mighty warrior and swift of foot, whereas the Tortoise was heavy and slow.“How big a head start do you need?” he asked the Tortoise with a smile.“Ten meters,” the latter replied.'
# this is our "testing"
how_many_times=10
for i in range(how_many_times):
  mylstm.eval() # put the model in evaluation
  print(string, end='') # print our string and let's see the output
  trying_string(string, 30, True, mylstm, num_layers, batch_size, hidden_size) # call our function 
  print() # print a space

# Take the face with the highest probability

#num_rolls = 5
#for i in range(num_rolls):
#   values, indices = torch.topk(dice, k=1, dim=-1)
#   print(indices)

#  printing our statistics
print("last epoch: ", best_epoch)
print("last perplexity: ", perplexity)
print("last loss: ", best_loss)

# PLOTTING
import matplotlib.pyplot as plt # import our library

plt.figure(figsize=(12,8)) # changing the pixels
print("perplexities: ") # proint our array of perplexities
print(perplexities)
print(len(perplexities))
print(count)
epoche=np.arange(0,count, 1) # defining epochs as a linespace (because we track the perplexity more times during the same epoch)
#print(epoche)
print(len(epoche))
plt.plot(epoche[100:], perplexities[100:]) # scatterplot

plt.xlabel("Count variable (at each 50 batches)") # label x
plt.ylabel("Perplexity") # label y
plt.tight_layout()
plt.show() # show everything